# BİM501:Yapay Sinir Ağları-1 ve BİM627:Derin Öğrenme-1 Dersleri İçin Güncellenen Ders İçeriği

**Prof. Dr. Alper Baştürk**\
Erciyes Üniversitesi, Mühendislik Fakültesi, Bilgisayar Mühendisliği
Bölümü\
38039 Melikgazi, Kayseri, Türkiye\
[**https://dr.alperbasturk.com**](https://dr.alperbasturk.com)\
[**alperbasturk@gmail.com**](mailto:alperbasturk@gmail.com)

2025-09-26

### Giriş: Dersin Felsefesi ve Ön Gereksinimler

#### Dersin Tanımı

Bu ders, yapay sinir ağları ve derin öğrenmenin teorik temelleri ile
pratik uygulamaları arasında derinlemesine bir köprü kurmayı
amaçlamaktadır. Öğrenciler, sinirsel hesaplamanın matematiksel
altyapısından başlayarak, bilgisayarlı görü ve doğal dil işleme gibi
karmaşık alanlarda günümüzün en ileri mimarilerini tasarlama, eğitme ve
değerlendirme yetkinliğine ulaşacaktır. Müfredat, yalnızca
algoritmaların "nasıl" çalıştığını değil, aynı zamanda büyük mimari
devrimlerin ardındaki teorik motivasyonları ve tarihsel bağlamı
irdeleyerek "neden" bu şekilde evrildiğini de vurgulamaktadır.

#### Ön Gereksinimler

Katılımcıların Lineer Cebir, Kalkülüs ve Olasılık teorisi konularında
sağlam bir matematiksel temele sahip olmaları beklenmektedir. Tüm
ödevler ve projeler, NumPy gibi temel kütüphaneler ve PyTorch veya
TensorFlow gibi modern bir derin öğrenme çerçevesi kullanılarak Python
programlama dilinde geliştirilecektir. Bu nedenle, öğrencilerin veri
yapıları ve algoritma tasarımı dahil olmak üzere orta düzeyde Python
bilgisine sahip olmaları zorunludur.

#### Öğrenim Çıktıları

Bu dersi başarıyla tamamlayan öğrenciler aşağıdaki yetkinlikleri
kazanacaktır:

- Yapay sinir ağlarının tarihsel gelişimini, dönüm noktalarını ve bu
  gelişmeleri yönlendiren teorik motivasyonları kapsamlı bir şekilde
  açıklayabilme.

- Farklı sinir ağı mimarilerini temel bileşenlerinden başlayarak
  (sıfırdan) uygulanabilme ve etkin bir şekilde eğitme.

- Aşırı öğrenme (overfitting) ve gradyanların kararsızlığı gibi derin
  öğrenme modellerinin eğitiminde karşılaşılan yaygın sorunları teşhis
  etme ve ileri düzey çözüm stratejileri geliştirebilme.

- Belirli bir problem alanı için en uygun mimariyi (CNN, RNN,
  Transformer vb.) eleştirel bir bakış açısıyla değerlendirip seçebilme
  ve bu seçimi teorik olarak gerekçelendirebilme.

- Modern büyük ölçekli modellerin altında yatan temel prensipleri anlama
  ve alandaki güncel araştırma literatürünü etkin bir şekilde takip edip
  yorumlayabilme.

# Bölüm I: Sinirsel Hesaplamanın Temelleri

### Hafta 1

#### Tarihin Yankıları: Biyolojik Nörondan Perceptron'a ve İlk "Yapay Zeka Kışı"

#### Haftalık Bakış

Bu açılış haftasında, yapay sinir ağlarının entelektüel kökenlerine
doğru bir yolculuğa çıkıyoruz. Biyolojik nöronları modelleme arzusundan
doğan ilk teorik adımlardan, öğrenme yeteneğine sahip ilk algoritma olan
Perceptron'a uzanan bu serüveni keşfedeceğiz. Alanın temel matematiksel
dilini oluşturacak, ilk "Yapay Zeka Kışı"na yol açan teorik
sınırlılıkları eleştirel bir gözle inceleyerek gelecekteki
inovasyonların zeminini hazırlayacağız.

#### Temel Konular ve Bütünleşik Araştırma

- **Biyolojik Esin Kaynakları ve Tarihsel Temeller (Biological
  Inspiration & Historical Foundations):** McCulloch ve Pitts'in 1943
  tarihli seminal çalışmasıyla [\[1\]](#ref-mcculloch1943) başlayan,
  beynin basit ve birbirine bağlı nöronlardan oluşan mimarisinin yapay
  zeka alanına nasıl ilham verdiğinin incelenmesi.

- **Perceptron Devrimi (The Perceptron):** Frank Rosenblatt'ın 1958'deki
  Perceptron modelinin [\[2\]](#ref-rosenblatt1958) bir dönüm noktası
  olarak analizi. Perceptron öğrenme algoritmasının ve doğrusal olarak
  ayrılabilir (linearly separable) veriler için ilişkileri öğrenme
  kapasitesinin matematiksel olarak açıklanması.

- **Teorik Duvar: Minsky ve Papert'in Eleştirisi (The Theoretical
  Barrier: Minsky & Papert's Critique):** Minsky ve Papert'in 1969
  tarihli *Perceptrons* adlı eserinin [\[3\]](#ref-minsky1969)
  derinlemesine incelenmesi. Bu çalışmanın, meşhur XOR mantıksal
  fonksiyonu ile sembolleşen, doğrusal olarak ayrılamayan problemleri
  çözme konusundaki temel yetersizliğini nasıl kanıtladığının analizi.

- **İlk "Yapay Zeka Kışı" (The First "AI Winter"):** Minsky ve Papert'in
  bu titiz matematiksel eleştirisinin, alandaki araştırma fonlarında ve
  bilimsel ilgide dramatik bir düşüşe yol açarak ilk yapay zeka kışını
  nasıl başlattığının anlaşılması.

- **Matematiksel Temellerin Gözden Geçirilmesi (Review of Mathematical
  Prerequisites):** Ders boyunca kullanılacak olan Lineer Cebir
  (vektörler, matrisler, iç çarpım) ve Kalkülüs (türevler, gradyanlar,
  zincir kuralı) gibi temel kavramların hızlı bir tekrarı.

Yapay sinir ağları tarihi, doğrusal bir ilerlemeden ziyade, diyalektik
bir döngü sergiler: Cesur bir yenilik (Perceptron), onu takip eden
keskin bir teorik eleştiri (Minsky ve Papert), bu eleştirinin
tetiklediği bir durgunluk dönemi ('Yapay Zeka Kışı') ve nihayetinde, bu
eleştirileri aşan devrimsel bir atılım (geri yayılım). Bu döngüyü
anlamak, alandaki ilerlemenin yalnızca artan hesaplama gücüyle değil,
aynı zamanda en zorlu teorik engellerle yüzleşip onları aşarak
sağlandığını kavramak için elzemdir.

#### Haftalık Araştırma Soruları

1.  Minsky ve Papert'in *Perceptrons* [\[3\]](#ref-minsky1969) adlı
    eserindeki eleştirinin, Rosenblatt'ın orijinal vizyonunun
    basitleştirilmiş bir yorumuna odaklandığı iddia edilmektedir.
    Rosenblatt'ın tasavvur ettiği çok katmanlı veya daha karmaşık
    algılayıcılar ile Minsky ve Papert'in analiz ettiği modeller
    arasındaki temel mimari ve felsefi farkları araştırınız.
    Rosenblatt'ın fikirleri, XOR probleminin üstesinden gelebilecek
    tohumları barındırıyor muydu?

2.  Doğrusal ayrılabilirlik kavramı, tek katmanlı perceptron'un
    aşamadığı bir engeldi. XOR'un ötesinde, doğası gereği doğrusal
    olarak ayrılamayan üç farklı gerçek dünya sınıflandırma problemini
    (örneğin, iç içe geçmiş daireler, spiral veri setleri) araştırıp
    matematiksel olarak tanımlayınız ve tek katmanlı bir perceptron'un
    bu problemleri neden çözemeyeceğini ispatlayınız.

3.  İlk "Yapay Zeka Kışı" teorik sınırlılıklar tarafından tetiklenmişti.
    1980'lerin sonu ve 1990'ların başındaki ikinci "Yapay Zeka Kışı"nın
    dinamiklerini araştırınız. Bu ikinci durgunluk dönemi de benzer
    teorik engellerden mi kaynaklanıyordu, yoksa hesaplama kapasitesinin
    yetersizliği, abartılı ticari vaatlerin karşılanamaması ve sembolik
    yapay zeka yaklaşımlarının hakimiyeti gibi faktörler mi daha
    belirleyiciydi?

4.  McCulloch ve Pitts'in 1943 tarihli nöron modeli
    [\[1\]](#ref-mcculloch1943), sinirsel aktivitenin mantıksal bir
    hesabı olarak kabul edilir. Bu modeli, derin öğrenmede kullanılan
    modern yapay nöron kavramıyla (ağırlıklı bir toplamın ardından gelen
    doğrusal olmayan bir aktivasyon fonksiyonu) karşılaştırarak temel
    kavramsal, matematiksel ve felsefi farklılıkları analiz ediniz.

5.  Hopfield Ağları (1982) [\[4\]](#ref-hopfield1982), istatistiksel
    mekanikten esinlenen bir teorik çerçeve sunarak ilk yapay zeka
    kışının sona ermesinde kilit bir rol oynamıştır. Bir Hopfield
    Ağı'nın mimarisini, enerji fonksiyonunu ve güncelleme kuralını
    araştırınız. Bellek ve hesaplamaya yönelik bu yaklaşım,
    Perceptron'un ileri beslemeli (feed-forward) doğasından nasıl
    temelden ayrışmaktadır?

### Hafta 2

#### İleri Beslemeli Sinir Ağları ve Aktivasyon Fonksiyonlarının Rolü

#### Haftalık Bakış

Bu hafta, derin öğrenmenin temel yapı taşı olan Çok Katmanlı
Perceptron'u (MLP) veya daha genel adıyla ileri beslemeli sinir ağını
(feed-forward neural network) inşa ederek, derin öğrenmenin kalbüne
ineceğiz. Tek katmanlı mimarilerin sınırlılıklarını aşarak, nöron
katmanlarını üst üste yığmanın karmaşık ve doğrusal olmayan
fonksiyonları nasıl temsil edebildiğini keşfedeceğiz. Ana odağımız,
derin ağlara asıl gücünü veren doğrusal olmamayı (non-linearity) sisteme
dahil eden aktivasyon fonksiyonlarının kritik rolü olacaktır.

#### Temel Konular ve Bütünleşik Araştırma

- **Çok Katmanlı Perceptron (MLP):** Girdi katmanı, bir veya daha fazla
  gizli katman ve bir çıktı katmanından oluşan temel ileri beslemeli ağ
  mimarisinin incelenmesi. Bilginin, matris çarpımları ve fonksiyon
  uygulamaları aracılığıyla girdiden çıktıya doğru nasıl aktığının
  anlaşılması.

- **Doğrusal Olmama Zorunluluğu (The Necessity of Non-linearity):**
  Kritik bir teorik kavrayış: Doğrusal olmayan aktivasyon fonksiyonları
  olmadan, çok katmanlı bir ağ, matematiksel olarak tek katmanlı bir
  doğrusal modele indirgenir ve bu da derinliğin tüm anlamını
  yitirmesine neden olur. Bu, Perceptron'un sınırlılığını doğrudan aşan
  temel adımdır.

- **Aktivasyon Fonksiyonları Panoraması (A Survey of Activation
  Functions):** Yaygın aktivasyon fonksiyonlarının, matematiksel
  özelliklerinin, avantaj ve dezavantajlarının kapsamlı bir analizi.

  - **Doygunluğa Ulaşan Fonksiyonlar (Saturating Functions):** Sigmoid
    (lojistik) ve Hiperbolik Tanjant (Tanh). Girdileri sonlu bir aralığa
    sıkıştırma özellikleri, tarihsel önemleri ve türevlerinin "kaybolan
    gradyan" (vanishing gradient) problemine
    [\[5\]](#ref-hochreiter1997) nasıl zemin hazırladığının
    tartışılması.

  - **Doygunluğa Ulaşmayan Fonksiyonlar (Non-saturating Functions):**
    Doğrultulmuş Doğrusal Birim (ReLU) [\[6\]](#ref-krizhevsky2012) ve
    Leaky ReLU, ELU, SELU gibi varyantları. ReLU'nun, hesaplama
    verimliliği ve pozitif girdiler için kaybolan gradyan problemini
    hafifletmesi sayesinde modern derin ağlar için neden varsayılan
    tercih haline geldiğinin anlaşılması.

  - **Çıktı Katmanı Fonksiyonları (Output Layer Functions):** Çok
    sınıflı sınıflandırma görevlerinde çıktıları bir olasılık dağılımına
    dönüştüren Softmax fonksiyonu. Regresyon görevleri için ise doğrusal
    (özdeşlik) fonksiyonun kullanımı.

- **Evrensel Yaklaşım Teoremi (Universal Approximation Theorem):** Sonlu
  sayıda nörona sahip, tek bir gizli katmanlı bir ileri beslemeli ağın,
  herhangi bir sürekli fonksiyonu istenen hassasiyette
  yaklaştırabileceğini ifade eden bu önemli teoremin
  [\[7\]](#ref-goodfellow2016) incelenmesi. Bu teorem, sinir ağlarının
  ifade gücünün teorik temelini oluşturur.

Aktivasyon fonksiyonlarının Sigmoid/Tanh'tan ReLU ve varyantlarına doğru
evrimi, sadece daha iyi doğrusal olmayanlıklar arayışının bir sonucu
değil, aynı zamanda derin ağları eğitmenin pratik zorluklarına verilmiş
doğrudan bir yanıttır. Her yeni fonksiyon, derin ağlarda öğrenmenin can
damarı olan gradyanların akışını iyileştirmek için tasarlanmış bir
mühendislik ve matematiksel çözümdür. Seçim keyfi değil, eğitim
dinamiklerini etkileyen kritik bir karardır.

#### Haftalık Araştırma Soruları

1.  Evrensel Yaklaşım Teoremi, tek bir gizli katmanın teorik olarak
    *yeterli* olduğunu garanti ederken, modern derin öğrenme neden
    onlarca, hatta yüzlerce katman kullanır? Tek bir, çok geniş gizli
    katman yerine derin, çok katmanlı mimariler kullanmanın pratik ve
    teorik avantajlarını (parametre verimliliği, hiyerarşik özellik
    öğrenimi, genelleme yeteneği vb.) araştırıp açıklayınız.

2.  GELU (Gaussian Error Linear Unit) aktivasyon fonksiyonu, BERT
    [\[8\]](#ref-devlin2018) gibi modern Transformer modellerinde
    standart hale gelmiştir. GELU'nun matematiksel formülünü ReLU ve ELU
    ile karşılaştırınız. GELU'nun, onu büyük ölçekli dil modelleri için
    özellikle uygun kılan stokastik ve deterministik özellikleri
    nelerdir?

3.  "Ölen ReLU" (Dying ReLU) probleminin ardındaki mekanizmayı
    derinlemesine inceleyiniz. Eğitim sırasında hangi koşullar (örneğin,
    yüksek öğrenme oranı, büyük negatif sapma değerleri) nöronların
    kalıcı olarak etkisiz hale gelme olasılığını artırır? Bu sorunu
    hafifletmek için tasarlanmış üç farklı ReLU varyantının (Leaky ReLU,
    PReLU, ELU) mekanizmalarını karşılaştırarak avantaj ve
    dezavantajlarını tartışınız.

4.  [\\(f(x) = x \\cdot \\text{sigmoid}(x)\\)]{.math .inline} olarak
    tanımlanan Swish (veya SiLU) aktivasyon fonksiyonu, birçok görevde
    güçlü bir performans göstermiştir. Matematiksel özelliklerini analiz
    ediniz. Hem doğrusal hem de doğrusal olmayan fonksiyonların
    unsurlarını nasıl birleştirir ve bu durum, standart bir ReLU'ya
    kıyasla gradyan akışı ve kayıp yüzeyinin (loss landscape)
    pürüzsüzlüğü açısından neden faydalı olabilir?

5.  Çoğu aktivasyon fonksiyonu eleman bazında (element-wise) uygulanan
    skaler fonksiyonlarken, Maxout aktivasyon fonksiyonu farklı bir
    yaklaşım sunar. Maxout biriminin nasıl çalıştığını ve ReLU
    fonksiyonunun genelleştirilmiş bir hali olarak nasıl
    görülebileceğini araştırınız. Model kapasitesi, hesaplama maliyeti
    ve "ölen nöron" problemine karşı dayanıklılığı açısından temel
    avantajları ve dezavantajları nelerdir?

# Bölüm II: Derin Sinir Ağlarının Eğitimi

### Hafta 3

#### Öğrenme Sanatı: Kayıp Fonksiyonları ve Optimizasyon

#### Haftalık Bakış

Bu hafta, öğrenme sürecinin temel mekanizması olan optimizasyona
derinlemesine bir dalış yapıyoruz. Eğitimin nihai amacını, bir kayıp
fonksiyonunu (loss function) en aza indirmek olarak resmileştireceğiz.
Temel algoritma olan gradyan inişini (gradient descent) ve derin öğrenme
pratiğinde kullanılan daha verimli ve ölçeklenebilir varyantlarını
keşfedeceğiz. Bu yolculuk, günümüzün karmaşık modellerini eğitmek için
vazgeçilmez olan modern, uyarlanabilir (adaptive) optimizasyon
algoritmalarının incelenmesiyle son bulacak.

#### Temel Konular ve Bütünleşik Araştırma

- **Amacın Tanımlanması: Kayıp Fonksiyonları (Loss Functions):** Bir
  kayıp fonksiyonunun [\[7\]](#ref-goodfellow2016), bir modelin tahmini
  ile gerçek değer arasındaki uyumsuzluğu ölçerek en aza indirilecek tek
  bir skaler değer sağlama rolünün incelenmesi.

  - **Regresyon Kayıpları (Regression Losses):** Ortalama Kare Hata
    (MSE) ve Ortalama Mutlak Hata (MAE). Özellikle MSE'nin aykırı
    değerlere (outliers) karşı hassasiyeti gibi pratik özelliklerinin
    tartışılması.

  - **Sınıflandırma Kayıpları (Classification Losses):** İki sınıflı
    problemler için İkili Çapraz Entropi (Binary Cross-Entropy) ve çok
    sınıflı problemler için Kategorik Çapraz Entropi (Categorical
    Cross-Entropy). Bu fonksiyonların, olasılık dağılımları arasındaki
    farkı nasıl ölçtüğünün anlaşılması.

- **Gradyan İnişi Algoritması (Gradient Descent Algorithm):** Kayıp
  fonksiyonunun gradyanını kullanarak model parametrelerini (ağırlıklar
  ve sapmalar) kaybı en hızlı şekilde azaltan yönde yinelemeli olarak
  güncelleme temel konsepti [\[9\]](#ref-rumelhart1986). Öğrenme oranı
  (learning rate) hiperparametresinin kritik önemi.

- **Gradyan İnişi Varyantları (Variants of Gradient Descent):**

  - **Toplu Gradyan İnişi (Batch Gradient Descent):** Tüm eğitim veri
    setini işledikten sonra ağırlıkları günceller. Büyük veri setleri
    için hesaplama açısından maliyetlidir.

  - **Stokastik Gradyan İnişi (Stochastic Gradient Descent - SGD):** Her
    bir eğitim örneğinden sonra ağırlıkları günceller. Daha hızlı
    güncellemeler yapar ancak eğitim sürecine gürültü/varyans ekler.

  - **Mini-Toplu Gradyan İnişi (Mini-Batch Gradient Descent):** Pratik
    bir uzlaşma. Küçük bir veri topluluğunu (batch) işledikten sonra
    ağırlıkları günceller, hesaplama verimliliği ile kararlı yakınsamayı
    dengeler.

- **Gelişmiş Optimizasyon Algoritmaları (Advanced Optimization
  Algorithms):** Standart SGD'nin zorluklarını (yerel minimumlar, dar
  vadiler) ele alan uyarlanabilir öğrenme oranı yöntemlerine giriş.

  - **Momentum:** Önceki güncelleme vektörünün bir kısmını mevcut olana
    ekleyerek SGD'yi ilgili yönde hızlandırmaya ve salınımları
    sönümlemeye yardımcı olur.

  - **RMSprop (Root Mean Square Propagation):** Her parametre için
    gradyanların karesinin hareketli bir ortalamasını tutar ve gradyanı
    bu ortalamaya bölerek öğrenme oranını parametre bazında etkili bir
    şekilde uyarlar.

  - **Adam (Adaptive Moment Estimation):** Birçok görev için fiili
    standart optimize edici [\[7\]](#ref-goodfellow2016). Her parametre
    için uyarlanabilir bir öğrenme oranı sağlamak üzere Momentum
    (birinci moment) ve RMSprop (ikinci moment) fikirlerini birleştirir.

Optimizasyon algoritmalarının SGD'den Adam gibi uyarlanabilir yöntemlere
doğru ilerlemesi, öğrenme oranının ayarlanmasını otomatikleştirme
yönünde açık bir eğilimi temsil etmektedir. Uyarlanabilir yöntemler, her
bir parametre için bireysel ve dinamik olarak uygun bir öğrenme oranı
bulmaya çalışır, bu da eğitim sürecini daha sağlam ve başlangıçtaki
öğrenme oranı seçimine daha az duyarlı hale getirir.

#### Haftalık Araştırma Soruları

1.  Adam optimize edicisi, momentum ve RMSprop'un en iyi yönlerini
    birleştirir. Orijinal Adam makalesini inceleyiniz. Birinci ve ikinci
    moment tahminleri için uygulanan "sapma düzeltme" (bias correction)
    adımları nelerdir ve bu adımlar özellikle eğitimin ilk aşamalarında
    neden kritik bir öneme sahiptir?

2.  Bir regresyon görevi için Ortalama Kare Hata (MSE) ve Ortalama
    Mutlak Hata (MAE) kayıp fonksiyonlarını karşılaştırınız. Aykırı
    değerlere (outliers) sahip bir veri setinde MAE'nin MSE'den daha
    uygun bir seçim olabileceği bir senaryo tanımlayınız ve bu
    tercihinizi gradyan özelliklerine dayanarak matematiksel olarak
    gerekçelendiriniz.

3.  Mini-Toplu Gradyan İnişi'nde toplu boyut (batch size) seçimi kritik
    bir hiperparametredir. Çok küçük toplu boyutları (örneğin, 1 veya 2)
    ile çok büyük toplu boyutları (örneğin, 1024 veya daha fazla)
    kullanmanın (a) yakınsama hızı, (b) eğitim sürecinin kararlılığı
    ve (c) son modelin genelleme performansı üzerindeki etkilerini
    araştırarak aralarındaki ödünleşimi (trade-off) açıklayınız.

4.  Menteşe Kaybı (Hinge Loss), Destek Vektör Makinelerini (SVM) eğitmek
    için popüler bir kayıp fonksiyonudur ve sinir ağlarında "maksimum
    marj" (maximum-margin) sınıflandırması için de kullanılabilir.
    Menteşe Kaybı'nı matematiksel olarak tanımlayınız ve sınıflar
    arasında bir ayrım marjını nasıl teşvik ettiğini açıklayınız.
    Amacını, olasılıksal bir yaklaşım sunan Çapraz Entropi Kaybı ile
    karşılaştırınız.

5.  Adagrad gibi optimize ediciler, öğrenme oranının çok agresif bir
    şekilde azalması ve eğitimin erken durması sorunundan muzdarip
    olabilir. Güncelleme kuralına dayanarak bu durumun nedenini
    açıklayınız. RMSprop algoritması, bu erken bozulmayı önlemek için
    Adagrad'ın güncelleme kuralını nasıl değiştirir?

### Hafta 4

#### Geri Yayılım Algoritması: Derin Öğrenmenin Motoru

#### Haftalık Bakış

Bu hafta, derin sinir ağlarını eğitmek için geliştirilmiş en önemli tek
algoritma olan Geri Yayılım'a (Backpropagation)
[\[9\]](#ref-rumelhart1986) ayrılmıştır. Algoritmayı, kalkülüsteki
zincir kuralının (chain rule) verimli ve özyinelemeli bir uygulamasından
ibaret olduğunu göstererek gizemini çözeceğiz. Öğrenciler, bir sinir
ağını bir hesaplama grafiği (computation graph) olarak görselleştirmeyi
ve gradyanların akışını kayıp fonksiyonundan başlayarak ağdaki her bir
parametreye geriye doğru nasıl izleyeceklerini öğreneceklerdir.

#### Temel Konular ve Bütünleşik Araştırma

- **Kavramsal Çerçeve (Conceptual Framework):** Geri yayılım, "hataların
  geriye doğru yayılması" olarak tanımlanır. Temel amacı, kayıp
  fonksiyonunun ağdaki her bir ağırlık ve sapmaya göre gradyanını (yani
  kısmi türevini) verimli bir şekilde hesaplamaktır.

- **Zincir Kuralı (The Chain Rule):** Geri yayılımın matematiksel motoru
  olarak çok değişkenli fonksiyonlar için zincir kuralının derinlemesine
  incelenmesi. Bir bileşik fonksiyonun türevinin, onu oluşturan
  fonksiyonların türevlerinin çarpılmasıyla nasıl hesaplanabildiğinin
  anlaşılması.

- **Hesaplama Grafları (Computation Graphs):** Bir sinir ağının ileri
  geçişini (forward pass) yönlendirilmiş döngüsüz bir işlem grafiği
  olarak temsil etme. Bu görselleştirme, gradyanlar grafiğin kenarları
  boyunca geriye doğru aktığı için zincir kuralının uygulanmasını
  sezgisel ve modüler hale getirir.

- **İleri ve Geri Geçiş (Forward and Backward Pass):**

  - **İleri Geçiş:** Veri, ağ boyunca katman katman akar, her düğümde
    hesaplamalar yapılır ve nihai bir kayıp değeri hesaplanır.

  - **Geri Geçiş:** Kayıptan başlayarak, gradyan hesaplanır ve graf
    boyunca katman katman geriye doğru yayılır. Her düğümde, "yerel
    gradyan" (local gradient) hesaplanır ve bir sonraki katmandan gelen
    "yukarı akış gradyanı" (upstream gradient) ile çarpılarak önceki
    katmanın gradyanı elde edilir.

- **Geri Yayılımın Verimliliği (Efficiency of Backpropagation):** Geri
  yayılımı, her parametre için gradyanı ayrı ayrı sayısal olarak tahmin
  eden naif bir yaklaşımla karşılaştırma. Geri yayılım, tüm gradyanları
  sadece bir ileri ve bir geri geçişte hesaplayarak çok daha verimli bir
  çözüm sunar.

Geri yayılım, bir dinamik programlama biçimi olarak da anlaşılabilir.
Karmaşık bir problemi (toplam gradyanı hesaplama), daha küçük,
özyinelemeli alt problemlere (her katmanda gradyanı hesaplama) ayırarak
çözer. Ara sonuçları (bir sonraki katmandaki gradyanlar) saklayıp
yeniden kullanarak gereksiz hesaplamaları önler. Bu, neden bu kadar
hesaplama açısından verimli olduğunu açıklamaya yardımcı olur ve modern
derin öğrenme çerçevelerinin modülerliğini vurgular.

#### Haftalık Araştırma Soruları

1.  Standart geri yayılım, yönlendirilmiş döngüsüz bir graf (DAG)
    üzerinde çalışır. Tekrarlayan Sinir Ağları (RNN'ler) için kullanılan
    Zaman İçinde Geri Yayılım (Backpropagation Through Time - BPTT)
    [\[7\]](#ref-goodfellow2016) algoritmasını araştırınız. BPTT, bir
    RNN'nin hesaplama grafiğindeki döngüleri (tekrarlayan bağlantıları)
    ele almak için standart geri yayılım algoritmasını nasıl uyarlar ve
    bu uyarlamanın getirdiği pratik zorluklar (örneğin, kesilmiş BPTT)
    nelerdir?

2.  Otomatik türev alma (automatic differentiation - "autodiff"),
    PyTorch ve TensorFlow gibi modern çerçevelerde geri yayılımı
    uygulayan temel teknolojidir. Autodiff'in iki ana modunu
    karşılaştırınız: ileri mod (forward mode) ve geri mod (reverse
    mode). Derin sinir ağlarını eğitmek için neden geri mod türev alma,
    özellikle çok sayıda parametreye sahip modellerde, ezici bir
    üstünlüğe sahiptir?

3.  Tanh aktivasyon fonksiyonuna sahip tek bir tam bağlantılı katman
    için geri yayılım güncelleme kurallarını (yani, kaybın ağırlıklara,
    sapmalara ve girdi aktivasyonlarına göre gradyanlarını) zincir
    kuralını kullanarak adım adım türetiniz.

4.  Ağırlık başlangıçlandırma (weight initialization) şeması, etkili bir
    eğitim için kritik öneme sahiptir. Bir ağdaki tüm ağırlıklar sıfıra
    veya aynı sabite başlatılırsa, geri yayılım sırasında gradyanlara ne
    olur? Bu durum, ağın simetriyi kıramamasına ve dolayısıyla
    öğrenmesini engellemesine nasıl yol açar?

5.  Evrişimli Sinir Ağlarında (CNN) geri yayılım, geri geçişte bir
    "transpoze evrişim" (transposed convolution) veya "dekonvolüsyon"
    işlemi içerir. Bir evrişimli katmanın girdisine göre geri geçişin
    neden başka bir evrişim işlemi olarak ifade edilebileceğini sezgisel
    olarak açıklayınız. İleri geçişte kullanılan filtre ile bu geri
    geçişte kullanılan filtre arasındaki matematiksel ilişki nedir?

# Bölüm III: Özel Veri Türleri İçin Mimariler

### Hafta 5

#### Genelleme Sanatı: Düzenlileştirme ve Aşırı Öğrenmeyle Mücadele

#### Haftalık Bakış

Eğitim verilerinde mükemmel performans gösteren ancak yeni, daha önce
görülmemiş verilerde başarısız olan bir model, pratik anlamda işe
yaramaz. Bu hafta, derin öğrenmenin en temel sorunlarından biri olan
**aşırı öğrenme (overfitting)** ile yüzleşeceğiz. Bir modelin
**genelleme (generalization)** yeteneğini artırmak için tasarlanmış ve
topluca **düzenlileştirme (regularization)** olarak bilinen bir dizi
güçlü tekniği keşfedeceğiz. Ağırlık cezalarından Dropout gibi stokastik
yöntemlere ve veri merkezli yaklaşımlara kadar geniş bir yelpazeyi ele
alacağız.

#### Temel Konular ve Bütünleşik Araştırma

- **Aşırı Öğrenme Problemi (The Problem of Overfitting):** Aşırı
  öğrenmeyi, bir modelin verinin altında yatan deseni öğrenmek yerine,
  gürültüsü de dahil olmak üzere eğitim verilerini ezberlemesi olarak
  tanımlama. Bu durumun, eğitim kaybı ile doğrulama/test kaybı arasında
  büyük bir farka yol açmasının nedenleri. Aşırı model karmaşıklığı veya
  yetersiz eğitim verisi gibi temel sebeplerin incelenmesi.

- **L1 ve L2 Düzenlileştirmesi (Ağırlık Azaltma - Weight Decay):**

  - **Mekanizma:** Kayıp fonksiyonuna, modelin ağırlıklarının
    büyüklüğüne dayalı bir ceza terimi ekleme
    [\[7\]](#ref-goodfellow2016). Toplam kayıp = [\\(\\text{Orijinal
    Kayıp} + \\lambda \\cdot \\text{Düzenlileştirme Terimi}\\)]{.math
    .inline} formülünün analizi.

  - **L2 Düzenlileştirmesi (Ridge):** Ceza teriminin, ağırlıkların
    *karelerinin* toplamı ([\\(\\lambda \\cdot \\sum(w_i\^2)\\)]{.math
    .inline}) olduğu durum. Bu yöntemin, daha küçük ve daha dağınık
    ağırlık değerlerini teşvik etmesinin, ancak ağırlıkları tam olarak
    sıfır yapmamasının incelenmesi.

  - **L1 Düzenlileştirmesi (Lasso):** Ceza teriminin, ağırlıkların
    *mutlak değerlerinin* toplamı ([\\(\\lambda \\cdot
    \\sum\|w_i\|\\)]{.math .inline}) olduğu durum. Bu yöntemin, bazı
    ağırlıkları tam olarak sıfıra indirerek etkili bir şekilde özellik
    seçimi (feature selection) yapması ve seyrek (sparse) modeller
    oluşturması.

- **Dropout:** Her eğitim iterasyonunda nöronların rastgele bir alt
  kümesinin geçici olarak "bırakıldığı" veya göz ardı edildiği, son
  derece etkili ve yaygın olarak kullanılan bir düzenlileştirme tekniği
  [\[10\]](#ref-srivastava2014).

  - **Mekanizma:** Bu tekniğin, nöronların birbirine aşırı derecede
    bağımlı hale gelmesini (co-adaptation) önleyerek ağı daha sağlam ve
    yedekli temsiller öğrenmeye nasıl zorladığının anlaşılması.
    Dropout'un, daha küçük ağlardan oluşan büyük bir topluluğu
    (ensemble) eğitmek olarak yorumlanması.

  - **Uygulama:** Dropout'un yalnızca eğitim sırasında aktif olması.
    Test/çıkarım (inference) sırasında tüm nöronların kullanılması,
    ancak çıktılarının dropout olasılığı ile ölçeklendirilmesi.

- **Erken Durdurma (Early Stopping):** Bir doğrulama setindeki
  (validation set) performansın iyileşmeyi durdurduğu anda eğitimin
  sonlandırıldığı basit ama etkili bir teknik. Modelin eğitim verilerine
  aşırı öğrenmeye devam etmesinin bu şekilde önlenmesi.

- **Veri Artırma (Data Augmentation):** Veri merkezli bir
  düzenlileştirme tekniği. Mevcut verilerin değiştirilmiş kopyalarını
  oluşturarak eğitim veri setinin boyutunu ve çeşitliliğini yapay olarak
  artırma.

  - **Bilgisayarlı Görü İçin:** Geometrik dönüşümler (döndürme, kırpma,
    çevirme, ölçekleme) ve renk uzayı dönüşümleri (parlaklık, kontrast)
    gibi teknikler.

  - **Doğal Dil İşleme İçin:** Geri çeviri (back-translation) ve
    eşanlamlı değiştirme (synonym replacement) gibi teknikler.

Farklı mekanizmalarına rağmen tüm düzenlileştirme teknikleri, model
karmaşıklığını cezalandırma ilkesi altında birleştirilebilir. Bu,
düzenlileştirmeyi bir dizi geçici hile koleksiyonundan, model
kapasitesini veri karmaşıklığına uyacak şekilde kontrol etmek için
tutarlı bir stratejiye dönüştürür.

#### Haftalık Araştırma Soruları

1.  L1 ve L2 düzenlileştirmesinin geometrik yorumu arasındaki temel
    farkı açıklayınız. Kısıtlama bölgeleri (constraint regions)
    açısından düşünüldüğünde, L1 cezasının (elmas/eşkenar dörtgen
    şeklinde bir bölge) neden seyrek çözümlere (bazı ağırlıkların tam
    sıfır olmasına) yol açarken, L2 cezasının (dairesel bir bölge) yol
    açmadığını görsel ve matematiksel olarak izah ediniz.

2.  Dropout, bir tür model topluluklaştırması (model ensembling) olarak
    yorumlanabilir [\[10\]](#ref-srivastava2014). Bu yorumu ayrıntılı
    olarak açıklayınız. Bu topluluğu oluşturan modeller nelerdir ve test
    zamanında aktivasyonların dropout olasılığı ile ölçeklendirilmesi,
    bu örtük topluluğun tahminini nasıl yaklaştırır?

3.  Toplu Normalleştirme (Batch Normalization) [\[11\]](#ref-ioffe2015),
    öncelikli olarak içsel kovaryant kaymasını (internal covariate
    shift) çözmek için tasarlanmış olsa da, düzenleyici bir etkiye sahip
    olduğu ve bazen Dropout ihtiyacını azalttığı bilinmektedir. Toplu
    Normalleştirme mekanizmasını araştırıp açıklayınız.
    Mini-toplulardaki ortalama ve varyansın gürültülü tahminlerinin
    neden bir düzenleyici olarak işlev gördüğünü tartışınız.

4.  Bilgisayarlı görü için Mixup ve CutMix gibi gelişmiş veri artırma
    tekniklerini araştırınız. Bu yöntemler, geleneksel geometrik veya
    renk dönüşümlerinden nasıl farklıdır ve modelin doğrusal olmayan
    davranışını yumuşatarak ve özellikler arasında daha yumuşak karar
    sınırları öğrenmesini teşvik ederek sağlamlığı nasıl
    iyileştirdiklerine dair altta yatan hipotez nedir?

5.  Düzenlileştirme gücü [\\(\\lambda\\)]{.math .inline}'nın seçimi,
    L1/L2 düzenlileştirmesi için kritik bir hiperparametredir. Belirli
    bir veri seti ve model için optimal bir [\\(\\lambda\\)]{.math
    .inline} değeri seçmek üzere çapraz doğrulama (cross-validation)
    gibi sistematik bir yöntemi tanımlayınız. [\\(\\lambda\\)]{.math
    .inline} değeri çok küçük veya çok büyük seçilirse, modelin varyans
    ve sapma (bias-variance) dengesi nasıl etkilenir?

### Hafta 6

#### Evrişimli Sinir Ağları (CNN'ler): Görüntülerin Dili

#### Haftalık Bakış

Bu hafta, bilgisayarlı görüyü (computer vision) kökünden değiştiren özel
bir mimari olan Evrişimli Sinir Ağlarını (Convolutional Neural
Networks - CNN'ler) [\[12\]](#ref-lecun1998) tanıtıyoruz. Temel yapı
taşları olan evrişimli (convolutional) ve havuzlama (pooling)
katmanlarını keşfedecek, görüntülerin uzamsal yapısından faydalanarak
hiyerarşik özellik temsillerini nasıl verimli bir şekilde öğrendiklerini
anlayacağız. Bu ilkeleri pratikte görmek için alana öncülük eden LeNet-5
ve AlexNet gibi tarihi CNN mimarilerini analiz edeceğiz.

#### Temel Konular ve Bütünleşik Araştırma

- **CNN'ler için Motivasyon (Motivation for CNNs):** Tam bağlantılı
  MLP'leri doğrudan görüntülere uygulamanın getirdiği zorluklar:
  parametre sayısındaki patlama, uzamsal bilginin kaybı ve öteleme
  değişmezliğinin (translation invariance) olmaması. CNN'lerin,
  görüntülerin 2D yapısından yararlanarak bu sorunları nasıl aştığının
  incelenmesi.

- **Temel Yapı Taşları (Core Building Blocks):**

  - **Evrişimli Katman (Convolutional Layer):** CNN'in merkezi bileşeni.
    Girdi hacmi boyunca kaydırılan bir dizi öğrenilebilir filtre
    (çekirdek/kernel) kullanarak yerel özellik tespiti gerçekleştirir.

    - **Anahtar Kavramlar:** Alıcı alan (receptive field), derinlik
      (depth), adım (stride) ve dolgu (padding).

    - **Parametre Paylaşımı (Parameter Sharing):** Aynı filtrenin tüm
      uzamsal konumlarda kullanılması, parametre sayısını dramatik bir
      şekilde azaltır ve özelliklerin konumlarından bağımsız olarak
      tespit edilmesini sağlar (öteleme değişmezliği).

  - **Havuzlama Katmanı (Pooling Layer / Subsampling):** Özellik
    haritalarının uzamsal boyutlarını azaltan, temsili küçük ötelemelere
    karşı daha sağlam hale getiren ve hesaplama maliyetini düşüren bir
    alt örnekleme işlemi (örneğin, Maksimum Havuzlama, Ortalama
    Havuzlama).

  - **Tam Bağlantılı Katman (Fully Connected Layer):** Genellikle ağın
    sonunda, evrişimli ve havuzlama katmanları tarafından çıkarılan üst
    düzey özelliklere dayanarak nihai sınıflandırmayı yapmak için
    kullanılır.

- **CNN Mimarisi (CNN Architecture):** Hiyerarşik bir özellik öğrenimi
  için evrişimli, aktivasyon (genellikle ReLU) ve havuzlama katmanlarını
  üst üste yığarak derin bir ağ oluşturma. Erken katmanların kenarlar ve
  renkler gibi basit özellikleri, daha derin katmanların ise şekiller ve
  nesneler gibi daha karmaşık desenleri nasıl öğrendiğinin anlaşılması.

- **CNN'lerde Geri Yayılım (Backpropagation in CNNs):** Gradyanların
  evrişimli ve havuzlama katmanları aracılığıyla nasıl hesaplandığını ve
  yayıldığını anlama. Bir evrişim işleminin geri geçişinin, bir
  transpoze evrişim (transposed convolution) olarak ifade edilmesi.

- **Bilgisayarlı Görü Uygulamaları (Applications in Computer Vision):**
  Görüntü sınıflandırma, nesne tespiti, anlamsal bölütleme, tıbbi
  görüntü analizi, yüz tanıma ve insan aktivitesi tanıma gibi geniş bir
  yelpazedeki uygulamaların incelenmesi.

CNN'lerin başarısı, mimarilerinin görsel veriler için mükemmel bir
şekilde uygun olan güçlü bir *tümevarımsal önyargı* (inductive bias)
içermesinden kaynaklanmaktadır: yerellik (locality) ve öteleme
değişmezliği (translation invariance). Bu mimari önyargı, hipotez
uzayını önemli ölçüde kısıtlar ve öğrenmeyi genel bir MLP'ye göre çok
daha verimli ve etkili hale getirir.

#### Haftalık Araştırma Soruları

1.  LeNet-5 [\[12\]](#ref-lecun1998) ve AlexNet
    [\[6\]](#ref-krizhevsky2012) mimarileri, CNN'lerin tarihinde iki
    büyük kilometre taşıdır. Mimarilerini (katman sayısı, filtre
    boyutları, aktivasyon fonksiyonları, düzenlileştirme teknikleri)
    karşılaştırınız. AlexNet'in, LeNet-5'i önemli ölçüde geride
    bırakarak 2012 ImageNet yarışmasını kazanmasını sağlayan temel
    mimari ve algoritmik yenilikler nelerdi?

2.  Google'ın Inception Ağı gibi modern mimarilerde kullanılan 1x1
    evrişim (bazen "ağ içinde ağ" - network in network olarak da
    adlandırılır) kavramını açıklayınız. 1x1 evrişim, kanal sayısını
    (derinliği) azaltmak ve hesaplama verimliliğini artırmak için nasıl
    kullanılabilir? Ayrıca, doğrusal olmayanlık ekleyerek modelin temsil
    gücünü nasıl artırır?

3.  "Transpoze evrişim" (transposed convolution), bazen yanıltıcı bir
    şekilde "dekonvolüsyon" olarak da adlandırılır. Bu operasyonun nasıl
    çalıştığını ve standart bir evrişim işleminden nasıl farklılaştığını
    açıklayınız. Anlamsal bölütleme (semantic segmentation) veya görüntü
    üretme gibi, düşük çözünürlüklü bir özellik haritasından yüksek
    çözünürlüklü bir çıktı üretmeyi gerektiren görevlerde neden kritik
    bir bileşendir?

4.  "Alıcı alan" (receptive field) kavramı, bir CNN'deki bir nöronun
    girdi görüntüsünün ne kadarını "gördüğünü" tanımlar. Aşağıdaki
    yapıya sahip bir ağın son özellik haritasındaki tek bir nöronun
    teorik alıcı alan boyutunu hesaplayınız: Girdi(32x32) -\>
    Conv1(filtre=3x3, adım=1, dolgu=0) -\> Pool1(filtre=2x2, adım=2) -\>
    Conv2(filtre=3x3, adım=1, dolgu=0).

5.  ResNet (Kalıntısal Ağ - Residual Network) mimarisi
    [\[13\]](#ref-he2016), çok daha derin ağların (100'den fazla katman)
    eğitilmesini mümkün kılmıştır. Kalıntısal veya "atlama" bağlantısı
    (skip connection) nedir ve bu mimari yenilik, kaybolan gradyan
    (vanishing gradient) problemini hafifleterek kimlik fonksiyonunu
    (identity function) öğrenmeyi nasıl kolaylaştırır?

### Hafta 7

#### Tekrarlayan Sinir Ağları (RNN'ler): Zamanın ve Sıranın İzinde

#### Haftalık Bakış

Bu hafta odağımızı uzamsal verilerden, metin ve zaman serisi gibi sıralı
(sequential) verilere kaydırıyoruz. Keyfi uzunluktaki dizileri işlemek
için bir "bellek" mekanizmasıyla tasarlanmış olan Tekrarlayan Sinir
Ağlarını (Recurrent Neural Networks - RNN'ler)
[\[7\]](#ref-goodfellow2016) tanıtıyoruz. Basit bir RNN'nin yapısını
analiz edecek, onun en kritik sınırlılıklarını---kaybolan ve patlayan
gradyan (vanishing and exploding gradients) problemlerini
[\[5\]](#ref-hochreiter1997)---teşhis edecek ve ardından bu zorlukların
üstesinden gelerek uzun menzilli bağımlılıkları (long-range
dependencies) yakalamak için tasarlanmış sofistike kapılı
mimarileri---LSTM ve GRU---keşfedeceğiz.

#### Temel Konular ve Bütünleşik Araştırma

- **Sıralı Verilerin Modellenmesi (Modeling Sequential Data):**
  Girdileri adım adım işleyebilen ve geçmiş bilgilere dair bir iç durum
  veya "bellek" tutabilen modellere duyulan ihtiyacın anlaşılması.

- **Basit Tekrarlayan Sinir Ağı (Simple RNN) Mimarisi:**

  - **Tekrarlayan Bağlantı (Recurrent Connection):** [\\(t\\)]{.math
    .inline} zamanındaki gizli durumun, hem [\\(t\\)]{.math .inline}
    zamanındaki girdinin hem de [\\(t-1\\)]{.math .inline} zamanındaki
    gizli durumun bir fonksiyonu olması.

  - **Zaman Adımları Arasında Parametre Paylaşımı (Parameter Sharing
    Across Time):** Dizideki her eleman için aynı ağırlık matrislerinin
    (W, U, V) kullanılması.

  - Ağın zaman içinde "açılması" (unrolling) ile bir hesaplama grafiği
    oluşturarak geri yayılımın (BPTT) nasıl uygulandığının anlaşılması.

- **RNN'lerde Gradyan Problemleri (Vanishing & Exploding Gradients in
  RNNs):** Basit RNN'ler için kritik bir başarısızlık modu.

  - **Neden:** Geri yayılım sırasında tekrarlayan ağırlık matrisinin
    birçok zaman adımı boyunca tekrarlı çarpımının, gradyanların üssel
    olarak sıfıra küçülmesine (kaybolan) veya üssel olarak sonsuza
    büyümesine (patlayan) neden olması.

  - **Sonuç:** Kaybolan gradyanların, uzak geçmişten gelen hata sinyali
    kaybolduğu için modelin uzun menzilli bağımlılıkları öğrenmesini
    engellemesi. Patlayan gradyanların ise eğitimi kararsız hale
    getirmesi.

- **Uzun Kısa Süreli Bellek (Long Short-Term Memory - LSTM) Ağları:**
  Kaybolan gradyan problemine karşı geliştirilmiş bir çözüm
  [\[5\]](#ref-hochreiter1997).

  - **Mimari:** Bilgiyi uzun diziler boyunca taşımak için ayrı bir
    *hücre durumu* (cell state) sunması, bu durumun bir taşıma bandı
    gibi hareket etmesi.

  - **Kapı Mekanizmaları (Gating Mechanisms):** Üç kapının (Unutma,
    Girdi, Çıktı) bilgi akışını kontrol etmesi. Bu kapıların, hücre
    durumundan ne zaman bilgi okunacağını, yazılacağını ve silineceğini
    öğrenen küçük sinir ağları olması.

- **Kapılı Tekrarlayan Birim (Gated Recurrent Unit - GRU):** LSTM'ye
  daha basit ve hesaplama açısından daha verimli bir alternatif
  [\[14\]](#ref-cho2014).

  - **Mimari:** Unutma ve girdi kapılarını tek bir "güncelleme
    kapısı"nda birleştirmesi ve hücre durumu ile gizli durumu
    birleştirmesi.

  - **Ödünleşimler (Trade-offs):** GRU'ların, LSTM'lere göre daha az
    parametreye sahip olması ve genellikle birçok görevde benzer
    performans göstermesi. Ancak LSTM'lerin çok uzun diziler üzerinde
    daha hassas bellek kontrolü gerektiren görevlerde avantajlı
    olabilmesi.

- **Doğal Dil İşleme Uygulamaları (Applications in NLP):** Dil
  modellemesi, makine çevirisi, duygu analizi, metin üretimi gibi
  alanlardaki uygulamaların incelenmesi.

LSTM ve GRU'nun temel yeniliği, hücre durumunun tekrarlı *çarpma*
yerine, öncelikle *toplamsal* etkileşimler yoluyla güncellenmesini
sağlamasıdır. Bu toplamsal yapı, gradyanların uzun mesafelerde
kaybolmadan akması için daha doğrudan ve kararlı bir yol oluşturarak
kaybolan gradyan problemini çözer.

#### Haftalık Araştırma Soruları

1.  Patlayan gradyan problemi, "gradyan kırpma" (gradient clipping) adı
    verilen basit bir teknikle ele alınabilir. Gradyan kırpmanın nasıl
    çalıştığını (örneğin, norm tabanlı kırpma) açıklayınız. Bu teknik,
    patlayan gradyanların altında yatan matematiksel sorunu mu çözer,
    yoksa sadece semptomları yöneterek eğitimin kararlılığını mı sağlar?

2.  Çift Yönlü RNN'ler (Bidirectional RNNs - Bi-RNN'ler), bir diziyi hem
    ileri hem de geri yönde işleyerek her bir zaman adımı için tam
    bağlamsal bilgi sağlar. Bir Bi-LSTM'in mimarisini açıklayınız. Hangi
    tür NLP görevleri için (örneğin, duygu analizi mi yoksa dil
    modellemesi mi) çift yönlü bir model, tek yönlü bir modele göre
    belirgin bir avantaj sağlar ve neden?

3.  LSTM mimarisinin üç kapısı vardır (unutma, girdi, çıktı). "Gözetleme
    deliği" (peephole) bağlantılarına sahip LSTM gibi yaygın LSTM
    varyantlarını araştırınız. Gözetleme deliği LSTM'ler, kapıların
    hesaplamasına hangi ek bilgiyi (hücre durumunu) dahil eder ve bu
    mimari değişikliğin arkasındaki motivasyon nedir?

4.  Basit bir RNN'nin gizli durumu için güncelleme denklemini türetiniz.
    Bir Tanh aktivasyon fonksiyonu ve T uzunluğunda bir dizi varsayarak,
    T zamanındaki kaybın 1. zamandaki gizli duruma göre gradyanının
    ifadesini zincir kuralı ile yazınız. Bu ifadenin, tekrarlayan
    ağırlık matrisinin tekrarlı çarpımı nedeniyle kaybolan veya patlayan
    gradyan potansiyelini nasıl gösterdiğini açıklayınız.

5.  GRU'lar genellikle LSTM'lerden daha hızlı eğitilir. İlgili
    güncelleme denklemlerini analiz ederek, her bir mimarinin bir zaman
    adımı başına gerektirdiği matris çarpımı sayısını karşılaştırınız.
    Hesaplamadaki bu fark, GRU'ların verimlilik kazanımlarını nasıl
    açıklar ve bu basitliğin modelin ifade gücü üzerindeki potansiyel
    etkileri nelerdir?

# Bölüm IV: Modern Derin Öğrenme Manzarası

### Hafta 8

#### Transformer Mimarisi ve Öz-Dikkat Devrimi

#### Haftalık Bakış

Bu hafta, modern doğal dil işlemeyi (NLP) tanımlayan mimariye, yani
Transformer'a [\[15\]](#ref-vaswani2017) devrimsel bir geçiş yapıyoruz.
"Attention Is All You Need" makalesinde [\[15\]](#ref-vaswani2017)
tanıtılan bu modeli, RNN'lerin sıralı işleme paradigmasından tamamen
uzaklaşarak temel bileşenlerine ayıracağız. Ana odağımız, modelin zengin
ve bağlamsal temsiller oluşturmak için bir dizideki tüm kelimelerin
önemini paralel olarak nasıl tartmasına olanak tanıyan öz-dikkat
(self-attention) mekanizması olacaktır.

#### Temel Konular ve Bütünleşik Araştırma

- **RNN'lerin Sınırlılıkları ve Transformer'lar için Motivasyon
  (Limitations of RNNs & Motivation for Transformers):** LSTM/GRU'lar
  basit RNN'leri geliştirmiş olsa da, doğaları gereği sıralı olmaları
  bir dizi içinde paralelleştirmeyi engeller. Bu durum, çok uzun
  belgelere ölçeklenmelerini zorlaştırır.

- **Öz-Dikkat Mekanizması (Self-Attention Mechanism):** Transformer'ın
  temel yeniliği. Belirli bir kelime için bir temsil üretirken, modelin
  girdi dizisindeki diğer tüm kelimelerin etkisini dinamik olarak
  tartmasına olanak tanır.

  - **Sorgular, Anahtarlar ve Değerler (Queries, Keys, and Values -
    QKV):** Her girdi jetonu (token) için üç vektör oluşturulur: bir
    Sorgu (ne arıyorum?), bir Anahtar (ne içeriyorum?) ve bir Değer (ne
    sağlıyorum?).

  - **Dikkat Puanı Hesaplaması (Attention Score Calculation):** İki
    jeton arasındaki dikkat puanı, alıcı jetonun Sorgu vektörü ile
    dikkat edilen jetonun Anahtar vektörünün iç çarpımı (dot product)
    alınarak hesaplanır. Bu puan, aralarındaki uyumluluğu temsil eder.

  - **Ölçeklendirilmiş İç Çarpım Dikkati (Scaled Dot-Product
    Attention):** Puanlar, gradyanları stabilize etmek için anahtar
    vektörlerinin boyutunun karekökü ile ölçeklendirilir, ardından
    dikkat ağırlıkları (bir olasılık dağılımı) oluşturmak için bir
    Softmax fonksiyonundan geçirilir.

  - **Çıktı (Output):** Bir jeton için nihai çıktı, dikkat ağırlıkları
    kullanılarak dizideki tüm jetonların Değer vektörlerinin ağırlıklı
    toplamıdır.

- **Çok Başlı Dikkat (Multi-Head Attention):** Öz-dikkat mekanizmasını,
  Q, K ve V için farklı, öğrenilmiş doğrusal projeksiyonlarla paralel
  olarak birden çok kez çalıştırma. Bu, modelin farklı konumlardaki
  farklı temsil alt uzaylarından (representation subspaces) gelen
  bilgilere ortaklaşa dikkat etmesini sağlar. Çıktılar birleştirilir ve
  tekrar yansıtılır.

- **Tam Transformer Mimarisi (The Full Transformer Architecture):**

  - **Konumsal Kodlamalar (Positional Encodings):** Modelde tekrarlama
    veya evrişim bulunmadığından, jetonların sırası hakkındaki bilgiler
    açıkça enjekte edilmelidir. Bu, girdi gömülmelerine (input
    embeddings) konumsal kodlama vektörleri eklenerek yapılır.

  - **Kodlayıcı-Kod Çözücü Yapısı (Encoder-Decoder Structure):**
    Orijinal Transformer, bir kodlayıcı yığınına (girdi dizisini işlemek
    için) ve bir kod çözücü yığınına (çıktı dizisini oluşturmak için)
    sahiptir ve bunlar çapraz dikkat (cross-attention) ile birbirine
    bağlanır.

  - **Diğer Bileşenler (Other Components):** Her blok içinde İleri
    Beslemeli Ağlar (Feed-Forward Networks), Kalıntısal Bağlantılar
    (Residual Connections) ve Katman Normalleştirmesi (Layer
    Normalization).

RNN'ler sıralılık için güçlü bir tümevarımsal önyargıya sahipken,
Transformer bu önyargıyı tamamen bir kenara bırakır. Temel varsayımı,
bir jetonun anlamının, bağlamındaki diğer tüm jetonlarla olan ağırlıklı
bir ilişkiler kümesi tarafından tanımlandığıdır. Bu "ilişkisel" önyargı,
kitlesel paralelleştirmeye olanak tanıyarak LLM devriminin kapısını
aralamıştır.

#### Haftalık Araştırma Soruları

1.  Orijinal Transformer [\[15\]](#ref-vaswani2017), sinüzoidal konumsal
    kodlamalar (sinusoidal positional encodings) kullanmıştır. Bu
    kodlamaların matematiksel formülünü araştırıp açıklayınız. Jeton
    konumlarını temsil etmek için, özellikle eğitim sırasında
    görülenlerden daha uzun dizileri işleme yetenekleri ve göreceli
    konum bilgisini kodlama potansiyelleri açısından, onları
    öğrenilebilir konumsal kodlamalara (learnable positional encodings)
    göre avantajlı kılan hangi özellikleri vardır?

2.  "Maskeli" çok başlı dikkat (masked multi-head attention) nedir ve
    Transformer mimarisinin kod çözücü (decoder) kısmında neden
    vazgeçilmezdir? Metin üretimi gibi otoregresif (autoregressive)
    görevler için eğitim sırasında modelin gelecekteki jetonlara bakarak
    "hile yapmasını" nasıl önler?

3.  "Attention Is All You Need" makalesi [\[15\]](#ref-vaswani2017),
    ölçeklendirilmiş iç çarpım dikkatini (scaled dot-product attention)
    tanıttı. Ölçeklendirme faktörünün (anahtar vektör boyutunun
    karekökü) arkasındaki teorik mantığı açıklayınız. Bu ölçeklendirme,
    Softmax fonksiyonunun gradyanlarını nasıl stabilize eder ve ihmal
    edilseydi, özellikle büyük boyutlu anahtar vektörleriyle eğitim
    sırasında ne gibi sorunlar ortaya çıkardı?

4.  Transformer'lar uzun menzilli bağımlılıkları yakalamada başarılı
    olsalar da, öz-dikkat mekanizmalarının dizi uzunluğuna göre karesel
    ([\\(O(n\^2)\\)]{.math .inline}) bir hesaplama ve bellek
    karmaşıklığı vardır. Bunun neden böyle olduğunu açıklayınız. Bu
    karesel karmaşıklığı azaltmaya çalışan bir "verimli Transformer"
    varyantını (örneğin, Linformer, Reformer, Longformer) araştırıp
    temel mekanizmasını tanımlayınız.

5.  Vision Transformer'lar (ViT'ler), Transformer mimarisini doğrudan
    görüntü sınıflandırmasına uygular. 2D bir piksel ızgarası olan bir
    görüntü, bir Transformer'ın işleyebileceği bir jeton (token)
    dizisine nasıl dönüştürülür? ViT'lerde, CNN'lerdeki gibi hiyerarşik
    bir özellik çıkarımı olmamasına rağmen, modelin uzamsal bilgiyi
    nasıl öğrendiğini ve jetonunun rolünü açıklayınız.

### Hafta 9

#### Büyük Dil Modelleri ve Transfer Öğrenme Paradigması

#### Haftalık Bakış

Transformer anlayışımızın üzerine inşa ederek, bu hafta bu mimarinin
devasa, önceden eğitilmiş Büyük Dil Modelleri (Large Language Models -
LLM'ler) oluşturmak için nasıl kullanıldığını araştırıyoruz. Modern
NLP'nin temel taşı olan ön eğitim (pre-training) ve ince ayar
(fine-tuning) paradigmasına odaklanacağız. Ardından, alana yön veren iki
öncü LLM ailesinin---Transformer kodlayıcısını kullanan BERT ve
Transformer kod çözücüsünü kullanan GPT---ayrıntılı bir mimari ve
felsefi karşılaştırmasını yapacağız.

#### Temel Konular ve Bütünleşik Araştırma

- **Transfer Öğrenme Paradigması (The Transfer Learning Paradigm):** Bir
  modeli devasa, genel bir veri kümesi (örneğin, tüm internet) üzerinde
  denetimsiz bir görevle önceden eğitme ve ardından bu bilgiyi daha
  küçük, göreve özgü etiketli bir veri kümesi üzerinde ince ayar yaparak
  kullanma kavramı.

- **BERT (Bidirectional Encoder Representations from Transformers):**

  - **Mimari:** Yalnızca Transformer mimarisinin kodlayıcı (encoder)
    yığınını kullanır [\[8\]](#ref-devlin2018).

  - **Eğitim Amacı:** Girdideki rastgele jetonların maskelendiği ve
    modelin hem soldan hem de sağdan (çift yönlü - bidirectional)
    bağlamı kullanarak bu maskelenmiş jetonları tahmin etmesi gereken
    Maskeli Dil Modellemesi (Masked Language Modeling - MLM)
    kullanılarak önceden eğitilmiştir. Ayrıca Sonraki Cümle Tahmini
    (Next Sentence Prediction - NSP) görevini de kullanır.

  - **Kullanım Alanları:** Derin çift yönlü bağlamı öğrendiği için BERT,
    duygu analizi, soru yanıtlama ve adlandırılmış varlık tanıma gibi
    anlama tabanlı görevler (Natural Language Understanding - NLU) için
    idealdir.

- **GPT (Generative Pre-trained Transformer):**

  - **Mimari:** Yalnızca Transformer mimarisinin kod çözücü (decoder)
    yığınını, maskeli öz-dikkat ile kullanır [\[16\]](#ref-radford2018).

  - **Eğitim Amacı:** Standart bir (nedensel veya otoregresif) dil
    modelleme amacı kullanılarak önceden eğitilmiştir: bir dizideki bir
    sonraki kelimeyi yalnızca önceki kelimelere dayanarak tahmin etme
    (tek yönlü, soldan sağa - unidirectional).

  - **Kullanım Alanları:** Üretken doğası, onu sohbet robotları,
    özetleme, hikaye yazma ve kod üretme gibi metin üretme görevleri
    (Natural Language Generation - NLG) için ideal kılar.

- **Mimari ve Felsefi Farklılıklar (Architectural and Philosophical
  Differences):** BERT'in kodlayıcı-tek, çift yönlü, MLM ile eğitilmiş
  modeline karşı GPT'nin kod çözücü-tek, tek yönlü, nedensel LM ile
  eğitilmiş modelinin doğrudan bir karşılaştırması. BERT bir "bağlam
  anlayıcısı" iken GPT bir "dizi tamamlayıcısı/üreticisi"dir.

- **İnce Ayar ve İstem Mühendisliği (Fine-Tuning vs. Prompting):** BERT
  gibi modelleri belirli görevler için ince ayar yapmaktan, GPT gibi
  modellerle sıfır vuruşlu (zero-shot) veya birkaç vuruşlu (few-shot)
  bir şekilde görevleri yerine getirmek için istem mühendisliği (prompt
  engineering) kullanmaya geçişin tartışılması.

BERT ve GPT arasındaki temel fark, sadece mimari seçimleri değil, aynı
zamanda bu seçimleri zorunlu kılan ön eğitim hedefleridir. Maskeli Dil
Modellemesi, çift yönlü bir kodlayıcıyı doğal bir uyum haline
getirirken; Nedensel Dil Modellemesi, otoregresif bir kod çözücüyü tek
mantıklı seçenek yapar. Bu, bir modeli eğittiğiniz görevin, onun optimal
mimarisini temelden şekillendirdiğini gösterir.

#### Haftalık Araştırma Soruları

1.  BERT, iki hedefle önceden eğitilmiştir: Maskeli Dil Modellemesi
    (MLM) ve Sonraki Cümle Tahmini (NSP). Daha sonraki araştırmalar
    (örneğin, RoBERTa makalesi), NSP hedefinin çok yardımcı olmadığını
    ve hatta zararlı olabileceğini öne sürmüştür. NSP görevi için
    orijinal motivasyonu ve sonraki araştırmaların neden bu görevi
    etkisiz bulduğunu (örneğin, görev zorluğu, konu tahmini ile pozisyon
    tahmininin karışması) araştırıp açıklayınız.

2.  GPT-3, "bağlam içi öğrenme" (in-context learning) veya "birkaç
    vuruşlu istem" (few-shot prompting) kavramını popülerleştirmiştir.
    Bu paradigma, BERT gibi modellerle kullanılan geleneksel "ince ayar"
    (fine-tuning) paradigmasından nasıl farklıdır? Her bir yaklaşımın
    pratik avantajları (veri gereksinimi, esneklik) ve dezavantajları
    (performans, tekrarlanabilirlik) nelerdir?

3.  T5 (Text-to-Text Transfer Transformer) modeli, her NLP görevini bir
    metinden metne problemi olarak çerçeveler (örneğin, duygu analizi
    için girdi "duygu: Bu film harikaydı" ve modelin "pozitif" metnini
    üretmesi gerekir). Bu birleşik çerçeveyi açıklayınız ve BERT'te
    kullanılan göreve özgü sınıflandırma başlığı (task-specific
    classification head) yaklaşımından nasıl farklı olduğunu belirtiniz.

4.  Hem BERT hem de GPT, genellikle WordPiece veya Byte-Pair Encoding
    (BPE) gibi bir alt kelime jetonlaştırma (subword tokenization)
    yöntemine dayanır. Bu alt kelime jetonlaştırma algoritmalarından
    birinin nasıl çalıştığını (örneğin, birleştirme operasyonları) adım
    adım açıklayınız. Büyük dil modelleri için neden kelime düzeyinde
    (büyük kelime dağarcığı, bilinmeyen kelime sorunu) veya karakter
    düzeyinde (uzun diziler, anlamsal birim eksikliği) jetonlaştırmaya
    tercih edilirler?

5.  GLM (General Language Model), "otoregresif boşluk doldurma"
    (autoregressive blank infilling) adlı bir ön eğitim hedefi
    kullanarak benzer boyutlardaki BERT, T5 ve GPT'den daha iyi
    performans gösterdiğini iddia etmektedir. Bu ön eğitim hedefini
    araştırınız. Hem maskeli dil modellemesinin (BERT gibi) hem de
    otoregresif üretimin (GPT gibi) güçlü yönlerini nasıl birleştirmeye
    çalışır?

### Hafta 10

#### Ufuk Çizgisi: İleri Konular ve Gelecek Yönelimler

#### Haftalık Bakış

Son haftamızda, denetimli sınıflandırma ve regresyonun ötesindeki diğer
büyük derin öğrenme paradigmalarını keşfetmek için perspektifimizi
genişletiyoruz. GAN'lar ve VAE'ler ile üretken modellemeye (generative
modeling) üst düzey bir giriş yapacak ve pekiştirmeli öğrenme
(reinforcement learning) dünyasına kısa bir bakış atacağız. Mevcut
araştırma sınırlarını, ortaya çıkan trendleri ve bu güçlü teknolojilerin
uygulanmasını çevreleyen kritik etik hususları tartışarak dersimizi
sonlandıracağız.

#### Temel Konular ve Bütünleşik Araştırma

- **Üretken Modelleme (Generative Modeling):** Yeni, sentetik veriler
  üretmek için bir veri dağılımı [\\(p(x)\\)]{.math .inline} öğrenme
  amacı.

  - **Üretken Çekişmeli Ağlar (Generative Adversarial Networks -
    GANs):** Bir Üreteci (sahte veri oluşturan) ve bir Ayırt Edici
    (gerçek ile sahteyi ayırt etmeye çalışan) arasındaki iki oyunculu
    oyuna giriş [\[17\]](#ref-goodfellow2014). Çekişmeli eğitim
    sürecinin, üreteci giderek daha gerçekçi veriler üretmeye nasıl
    yönlendirdiğinin anlaşılması.

  - **Varyasyonel Otomatik Kodlayıcılar (Variational Autoencoders -
    VAEs):** Olasılıksal bir gizli uzay (latent space) öğrenen otomatik
    kodlayıcıların bir uzantısı [\[18\]](#ref-kingma2013). Girdiyi gizli
    uzayda bir dağılıma eşleyen bir kodlayıcıdan ve bu uzaydan örnekleme
    yaparak veri üreten bir kod çözücüden oluşması. VAE'lerin pürüzsüz
    gizli temsiller öğrenmedeki başarısı, ancak GAN'lardan daha bulanık
    görüntüler üretme eğilimi.

- **Pekiştirmeli Öğrenmeye (Reinforcement Learning - RL) Giriş:**

  - Bir ajanın (agent), birikimli bir ödül sinyalini en üst düzeye
    çıkarmak için bir ortamda (environment) kararlar almayı öğrendiği
    paradigma [\[19\]](#ref-sutton2018).

  - RL'nin sinir ağlarıyla paralel olarak tarihsel gelişimine ve modern
    Derin Pekiştirmeli Öğrenme'deki (örneğin, AlphaGo
    [\[20\]](#ref-silver2017)) birleşimlerine kısa bir değinme.

  - Daha fazla çalışma için Sutton ve Barto'nun
    [\[19\]](#ref-sutton2018) temel metnine referans.

- **Mevcut Araştırma Sınırları (Current Research Frontiers):** Kendi
  kendine denetimli öğrenme (self-supervised learning), çok modlu
  öğrenme (multimodal learning - metin, görüntü ve sesi birleştirme) ve
  daha verimli, ölçeklenebilir model mimarileri arayışı gibi ortaya
  çıkan konuların kısa bir tartışması.

- **Derin Öğrenmede Etik Hususlar (Ethical Considerations in Deep
  Learning):** Eğitim verilerindeki önyargı (bias) sorunları, üretken
  modellerin kötüye kullanım potansiyeli (deepfakes), model
  yorumlanabilirliği ve adaleti (fairness) ve büyük modelleri eğitmenin
  çevresel maliyeti de dahil olmak üzere derin öğrenmenin toplumsal
  etkisine dair eleştirel bir tartışma.

En gelişmiş yapay zeka sistemleri (örneğin, AlphaGo) saf bir derin
öğrenme türü değil, denetimli öğrenme, üretken modelleme ve pekiştirmeli
öğrenme unsurlarını birleştiren hibrit sistemlerdir. Bu, yapay zekanın
geleceğinin tek bir mimariyi mükemmelleştirmekte değil, karmaşık
sorunları çözmek için bu farklı öğrenme paradigmalarının
bütünleştirilmesinde yattığını göstermektedir.

#### Haftalık Araştırma Soruları

1.  VAE'ler ve GAN'ların eğitim hedeflerini, üretilen örnek kalitesini
    ve eğitim kararlılığını karşılaştırınız. GAN'lar neden tipik olarak
    daha keskin ve gerçekçi görüntüler üretirken, VAE'ler "mod çökmesi"
    (mode collapse) sorununa daha az eğilimlidir ve daha çeşitli
    çıktılar üretir?

2.  "Derin Pekiştirmeli Öğrenme" (Deep Reinforcement Learning - DRL)
    kavramını araştırınız. Derin bir sinir ağının (örneğin, bir CNN),
    Derin Q-Ağları (Deep Q-Networks - DQN) gibi bir algoritmada, yüksek
    boyutlu durum uzaylarını (örneğin, bir oyun ekranının pikselleri)
    işleyerek Q-değer fonksiyonu için bir fonksiyon yaklaştırıcı
    (function approximator) olarak nasıl kullanılabileceğini
    açıklayınız.

3.  Difüzyon Modelleri (Diffusion Models), son zamanlarda görüntü
    üretiminde son teknoloji sonuçlar elde eden yeni bir üretken model
    sınıfıdır. Bir difüzyon modelindeki ileri (gürültü ekleme -
    forward/diffusion process) ve geri (gürültü giderme -
    reverse/denoising process) süreçlerini üst düzeyde açıklayınız. Bu
    modellerin GAN'lara kıyasla avantajları (eğitim kararlılığı,
    çeşitlilik) ve dezavantajları (çıkarım hızı) nelerdir?

4.  "Önyargı girer, önyargı çıkar" (bias in, bias out), etik yapay zeka
    ile ilgili yaygın bir ifadedir. Eğitim verilerindeki toplumsal
    önyargılar nedeniyle zararlı veya adil olmayan sonuçlar üreten
    gerçek dünyadan bir derin öğrenme modelinin (örneğin, işe alım, adli
    tahmin, yüz tanıma sistemleri) vaka çalışmasını araştırıp sununuz.
    Bu önyargıyı azaltmak için veri toplama, modelleme veya dağıtım
    aşamalarında hangi adımlar atılabilirdi?

5.  GPT-3 gibi büyük modellerin eğitimi önemli bir karbon ayak izine
    sahiptir. "Yeşil Yapay Zeka" (Green AI) veya "Verimli Derin Öğrenme"
    (Efficient Deep Learning) alanını araştırınız. Araştırmacıların
    büyük modelleri eğitme ve dağıtma maliyetini (hesaplama, enerji,
    bellek) azaltmak için kullandığı üç farklı stratejiyi (örneğin,
    mimari değişiklikler, budama (pruning), niceleme (quantization),
    bilgi damıtma (knowledge distillation)) tanımlayınız.

### Başlıca Derin Öğrenme Mimarilerinin Karşılaştırmalı Analizi

| Mimari | Temel Yapı Taşı | Birincil Veri Türü | Temel Güçlü Yönler | Temel Sınırlılıklar |
|---------|-------------------|-------------------|-------------------|-------------------|
| **MLP** | Tam Bağlantılı Katman | Tablosal / Vektör Verileri | Evrensel fonksiyon yaklaştırıcı; basit ve esnek. | Parametre açısından verimsiz; uzamsal veya zamansal yapıyı göz ardı eder; yüksek boyutlu verilerle zorlanır. |
| **CNN** | Evrişim ve Havuzlama Katmanı | Izgara Benzeri Veriler (Görüntüler, Spektrogramlar) | Ağırlık paylaşımı yoluyla parametre açısından verimli; öteleme değişmezliği; uzamsal hiyerarşileri öğrenir. | Yerellik için güçlü tümevarımsal önyargı, uzun menzilli, yerel olmayan bağımlılıkları yakalamada daha az etkili. |
| **RNN (LSTM/GRU)** | Kapılı Tekrarlayan Hücre | Sıralı Veriler (Metin, Zaman Serisi) | Değişken uzunluktaki dizileri işler; geçmiş bilgilere dair durum/bellek tutar. | Doğası gereği sıralı işleme paralelleştirmeyi engeller; çok uzun menzilli bağımlılıklarla zorlanır. |
| **Transformer** | Öz-Dikkat Mekanizması | Küme/Dizi Verileri (Metin, Genomlar) | Tüm diziyi paralel olarak işler; uzun menzilli, küresel bağımlılıkları yakalamada üstündür. | Dizi uzunluğunda karesel karmaşıklık; dizi sırasını yerleşik olarak anlamaz. |

## Kaynaklar

**\[1\]** W. S. McCulloch and W. Pitts, "A logical calculus of the ideas
immanent in nervous activity," *The Bulletin of Mathematical
Biophysics*, vol. 5, no. 4, pp. 115--133, 1943.

**\[2\]** F. Rosenblatt, "The perceptron: a probabilistic model for
information storage and organization in the brain," *Psychological
Review*, vol. 65, no. 6, p. 386, 1958.

**\[3\]** M. Minsky and S. Papert, *Perceptrons: An Introduction to
Computational Geometry*, Cambridge, MA, USA: MIT Press, 1969.

**\[4\]** J. J. Hopfield, "Neural networks and physical systems with
emergent collective computational abilities," *Proceedings of the
National Academy of Sciences*, vol. 79, no. 8, pp. 2554--2558, 1982.

**\[5\]** S. Hochreiter and J. Schmidhuber, "Long short-term memory,"
*Neural Computation*, vol. 9, no. 8, pp. 1735--1780, 1997.

**\[6\]** A. Krizhevsky, I. Sutskever, and G. E. Hinton, "ImageNet
classification with deep convolutional neural networks," in *Advances in
Neural Information Processing Systems 25*, 2012, pp. 1097--1105.

**\[7\]** I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*,
Cambridge, MA, USA: MIT Press, 2016.

**\[8\]** J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT:
Pre-training of deep bidirectional transformers for language
understanding," in *Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers)*, 2019, pp.
4171--4186.

**\[9\]** D. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Learning
representations by back-propagating errors," *Nature*, vol. 323, no.
6088, pp. 533--536, 1986.

**\[10\]** N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, "Dropout: A simple way to prevent neural networks from
overfitting," *The Journal of Machine Learning Research*, vol. 15,
no. 1, pp. 1929--1958, 2014.

**\[11\]** S. Ioffe and C. Szegedy, "Batch normalization: Accelerating
deep network training by reducing internal covariate shift," in
*Proceedings of the 32nd International Conference on Machine Learning*,
2015, pp. 448--456.

**\[12\]** Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner,
"Gradient-based learning applied to document recognition," *Proceedings
of the IEEE*, vol. 86, no. 11, pp. 2278--2324, 1998.

**\[13\]** K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning
for image recognition," in *Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition*, 2016, pp. 770--778.

**\[14\]** K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio, "Learning phrase representations
using RNN encoder-decoder for statistical machine translation," in
*Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP)*, 2014, pp. 1724--1734.

**\[15\]** A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need,"
in *Advances in Neural Information Processing Systems 30*, 2017, pp.
5998--6008.

**\[16\]** A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,
"Improving language understanding by generative pre-training," OpenAI,
Tech. Rep., 2018.

**\[17\]** I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative
adversarial nets," in *Advances in Neural Information Processing Systems
27*, 2014, pp. 2672--2680.

**\[18\]** D. P. Kingma and M. Welling, "Auto-encoding variational
bayes," *arXiv preprint arXiv:1312.6114*, 2013.

**\[19\]** R. S. Sutton and A. G. Barto, *Reinforcement Learning: An
Introduction*, 2nd ed., Cambridge, MA, USA: MIT Press, 2018.

**\[20\]** D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen,
T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and
D. Hassabis, "Mastering the game of Go without human knowledge,"
*Nature*, vol. 550, no. 7676, pp. 354--359, 2017.
